{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "lonely-commodity",
   "metadata": {},
   "source": [
    "# Comparing two integrators in the usecase of HMC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "played-acquisition",
   "metadata": {},
   "source": [
    "In this blog post we will compare the performance of two different integrators, sampling with Hamiltonian Monte Carlo (HMC). In HMC, the classical Leapfrog is still the state of the art method to generate new proposal states for the Metropolis Hastings algorythm. In [this paper](https://arxiv.org/pdf/2007.05308.pdf) Jun Hao Hue et al. benchmark the performance of leapfrog and U7, which was first presented by Thanh Tri Chau et al [2018 New J. Phys. 20 073003](https://iopscience.iop.org/article/10.1088/1367-2630/aacde1/pdf), against variousclassical and quantum systems, but not against the sampling methods used in HMC.\n",
    "This blog post aims to give you first a brief introduction to HMC, then describes formally the two integrators and then numerically evaluates their performance. In case you are completely new to HMC or MCMC methods, a good starting point might be the [series of blog posts](https://www.tweag.io/blog/2019-10-25-mcmc-intro1/) by Simeon Carstens. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c48420",
   "metadata": {},
   "source": [
    "# Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ba5399",
   "metadata": {},
   "source": [
    "Broadly speaking, the idea of HMC is, that given a previous state $x$ of our markov chain, we draw a random momentum $v$ from a normal distribution and simulate the behaviour of a fictive particle with starting point $(x,v)$. This intuition makes sense, because our probability density gives rise to a potential energy and vice versa \\fotenote Simeons_blog_post. This deterministic behaviour is simulated for some fixed time $t$. The final state $(x^\\star, v^\\star)$ of the particle after some time t will then serve as the new proposal state of the Metropolis-Hastings algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad756bc0",
   "metadata": {},
   "source": [
    "You might wonder, why we can draw the initial momentum $v$ from a normal distribution. You can check yourself, if you have a closer look at the joint probability "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d78335",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    " p(x,v) \\propto & \\,\\exp \\{-H(x,v)\\} \\\\\n",
    "= & \\,\\exp \\{-K(v)\\} \\times \\exp \\{-E(x)\\}\\\\\n",
    " =& \\,\\exp \\{-\\frac{|v|^2}{2} \\} \\times p(x).\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5823b66",
   "metadata": {},
   "source": [
    "By definition, the two variables are independent from each other, therefore we can just sample $v$ independently of $x$. Moreover, you can see the marginal distribution of $v$ is just the normal distribution up to some constant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4627a7a0",
   "metadata": {},
   "source": [
    "The Hamiltonian is then given by $$H(x,v) = K(v) + E(x).$$ Usually the Hamiltonian does not need to be of the form kinetic energy plus potential energy (seperable), but it certainly makes things a lot easier and is a reasonable assumption for most usecases arising from Bayesian statistics and beyond. In fact, you can actually apply splitting methods, which we will do in the next steps, to nonseperable Hamiltonians as well, though some more math will be necessary. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e89fb6",
   "metadata": {},
   "source": [
    "As mentioned aboved, the key concept of the HMC method is to simulate the fictive particles dynamics. In the Hamiltonian formalism, this is usually done by solving a set of differential equations, namely\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60ec9fc",
   "metadata": {},
   "source": [
    "$$\\frac{dx}{dt} = \\frac{\\partial H }{\\partial v}, \\hspace{15pt} \\frac{dv}{dt}= -\\frac{\\partial H }{\\partial x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0627045",
   "metadata": {},
   "source": [
    "This system of coupled differential equations is formally solved by"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecaf9f6",
   "metadata": {},
   "source": [
    "\n",
    "\\begin{equation}\n",
    "\\begin{pmatrix}x \\\\v\\end{pmatrix} = e^{-tH_v} \\begin{pmatrix}x \\\\v\\end{pmatrix} ,\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3889f0a9",
   "metadata": {},
   "source": [
    "where we define $H_v=\\frac{\\partial H}{\\partial x}\\frac{\\partial}{\\partial v}-\\frac{\\partial H}{\\partial v}\\frac{\\partial}{\\partial x}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325d59bf",
   "metadata": {},
   "source": [
    "This notation can be simplified by introducing $z=(x,v)$. Then the Hamiltonian equations of motion can be rewritten in a single expression as "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84712dd7",
   "metadata": {},
   "source": [
    "$$\\dot{z} = \\{z,H(z)\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95440b48",
   "metadata": {},
   "source": [
    "where $\\{\\cdot, \\cdot \\}$ is a Poisson bracket. Furthermore, by introducing an operator $D_{H\\cdot} = \\{ \\cdot, H \\}$, the equation can be further simplified to "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592b57c4",
   "metadata": {},
   "source": [
    "$$\\dot z = D_H z$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a039b2",
   "metadata": {},
   "source": [
    "and is solved by $z(t) = \\exp ( t D_H) z(0) = \\exp ( t(D_K + D_E)) z(0)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alone-johnston",
   "metadata": {},
   "source": [
    "#### But wait!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8eca650",
   "metadata": {},
   "source": [
    "I dont want to loose any of you right on the spot, just because I used the term differential equation. It is actually much simpler than that, so don't worry. Just let me show you one simple remarkable result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0bc8c2",
   "metadata": {},
   "source": [
    "In our case of the seperable Hamiltonian, we can rewrite the differential equations, by just using the terms, that are actually dependend on the variable, we want to take the derivativ of, so\n",
    "$$\\frac{dx}{dt} = \\frac{\\partial}{\\partial v} K(v) = \\frac{v}{m}, \\hspace{15pt} \\frac{dv}{dt}= -\\frac{\\partial}{\\partial x} E(x).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e49d74",
   "metadata": {},
   "source": [
    "This might look already much more familiar to you. Just multiply the left equation with m and take once more the derivate with respect to $t$ gives us $m\\frac{d^2x}{dt^2}  = \\frac{dv}{dt}.$ This then plugt into the second equation \n",
    "$$m\\frac{d^2x}{dt^2}  = \\frac{dv}{dt} = -\\frac{\\partial}{\\partial x} E(x) = F$$\n",
    "leaves us with Newton's beautiful second law of motion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93355b83",
   "metadata": {},
   "source": [
    "## Leapfrog\n",
    "\n",
    "For now, let's come back to our fictive particle. As so often the differential eqaution is actually not always analytically tractable and so we need a way to approximate the behaviour of our fictive particle. And this is where the leapfrog method comes into play. The intuition is, that we update the space coordinate $x$ and the momentum variable $v$ one after an other, which is the behaviour, where the name comes from.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5dbdf1",
   "metadata": {},
   "source": [
    "![](Leapfrog.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf368e52",
   "metadata": {},
   "source": [
    "More rigorously, the updates look like the following,\n",
    "$$x_{i+1}= x_n +   v_{i + 1/2}\\Delta t$$\n",
    "$$v_{i + 3/2} = v_{i+1/2} + \\left(-\\frac{\\partial}{\\partial x} E(x_{i+1})\\right) \\Delta t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11298186",
   "metadata": {},
   "source": [
    "As you might have noticed, you need to perform half a step for the momentum in the beginning and the end. If we further devide our time $t$ into $t = stepsize * trajectory\\_length$, then a small function, approximating the desired behaviour, would have the following look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d1cf29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate(x, v):\n",
    "\n",
    "    v += 1./2 * stepsize * -gradient_pot_energy(x)\n",
    "\n",
    "    for i in range(trajectory_length-1):\n",
    "        x += stepsize * v\n",
    "        v += stepsize * gradient_pot_energy(x)\n",
    "\n",
    "    x += stepsize * v\n",
    "    v += 1./2 * stepsize * gradient_pot_energy(x)\n",
    "\n",
    "    return x, v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b90d16f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c60499f",
   "metadata": {},
   "source": [
    "## U7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03181509",
   "metadata": {},
   "source": [
    "The novelty of the U7 consists of the usage of the second order derivative of the potential energy. This comes also with a few more updates of $x$ and $v$ per stepsize and yields to a much lower error. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854e78e6",
   "metadata": {},
   "source": [
    "<!--\n",
    "Let me try, to give you an intuition in what way the second derivative (Hessian) is used. First lets have a closer look on the Leapfrog. As shown in the illustration above, you can think of the small lower index for $x$ and $v$ as the time. Now, lets say, we want to update the postition $x_i$ to $x_{i+1}$ next. We would use the previously calculated momentum $v_{i+1/2}$, which you can think of as the momentum of the particle at time $i+1/2$. To update the momentum, we have used the gradient of the potential energy at position $x_i$. The point is, that we want the momentum at time $i+1/2$, but use the position of the particle at time $i$ to update it. So instead, the U7 takes the position at time $i$ and adds to it somfou\n",
    "-->\n",
    "comment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d88a879",
   "metadata": {},
   "source": [
    "To get a grasp on why the U7 works how it works, we will introduce it from the perspective of quantum mechanics. In quantum mechanics the coordinates $x$ are replaced by the position operator $R$ and the momentum $v$ is replaced by the operator $P$, therefore a single particle Hamiltonian reads like this, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6192743",
   "metadata": {},
   "source": [
    "$$H = \\frac{P^2}{2}+V(R)$$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d38549",
   "metadata": {},
   "source": [
    "The Schr√∂dinger equation is then solved by"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65a8c7e",
   "metadata": {},
   "source": [
    "$$\\Psi (t) = e^{-\\frac{it}{\\hbar}H}\\Psi (0) = U(t) \\Psi (0), $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9ed9df",
   "metadata": {},
   "source": [
    "where $U(t)$ is the time-evolution operator. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca3b355",
   "metadata": {},
   "source": [
    "Suzuki http://people.ucalgary.ca/~dfeder/535/suzuki.pdf proposed a series of increasingly accurate approximation for the time-evolution operator of the form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2a1872",
   "metadata": {},
   "source": [
    "$$U_N = \\prod_{i=1}^{N/2} e^{\\frac{it}{\\hbar}\\alpha_iV(R)}e^{-\\frac{it}{\\hbar}\\beta_i\\frac {P^2}{2}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174e3942",
   "metadata": {},
   "source": [
    "One realization is the U7, which uses a special property, such that it is reduced to a five factor approximation, but remains an accuracy wich error in the $\\cal{O}(t^5)$ terms.\n",
    "That property has the following look:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27649a69",
   "metadata": {},
   "source": [
    "$$\\exp{\\left( -\\frac{itz}{2\\hbar}V(R)\\right)}\\exp{\\left( -\\frac{it}{3\\hbar z^2}P^2\\right)}\\exp{\\left( -\\frac{itz}{2\\hbar}V(R)\\right)} = \\exp{\\left( -\\frac{it}{3\\hbar z^2}\\left[ P + \\frac{1}{2}\\nabla V(R) \\right]^2\\right)} \\overset{z\\rightarrow \\infty}{\\longrightarrow}\\exp{\\left( -\\frac{it}{12\\hbar}[t\\nabla V(R)]^2\\right)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee908c6",
   "metadata": {},
   "source": [
    "One interesting remark is that, for symmetric approximations, $U(t)U(-t) = 1$, the error terms can't be of even order since then, intuitively speaking, the error would point in the same direction, because $t^{2n} = (-t)^{2n}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44743dee",
   "metadata": {},
   "source": [
    "This quantum mechanical algorythm has a quite simple representation in classical mechanics. Since $D_K^2 z = \\{\\{z,K\\}K\\} = \\{(\\dot  q, 0), K  \\} = (0,0)$ and $\\exp (a D_K) = \\Sigma_{n=0}^{\\infty} \\frac{(a D_K)^n}{n!} = 1+ aD_K$, that makes $\\exp(\\beta_i t D_K)$ the mapping "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2754e0",
   "metadata": {},
   "source": [
    "$$\\begin{pmatrix}x \\\\v\\end{pmatrix} \\mapsto \\begin{pmatrix}x + t \\beta_i \\frac{\\partial K}{\\partial p} (p)\\\\v\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4353993e",
   "metadata": {},
   "source": [
    "A simplified python method of the algorythm described above would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba479f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate(x, v):\n",
    "\n",
    "    v += 1./6 * stepsize * gradient_pot_energy(x)\n",
    "\n",
    "    for i in range(trajectory_length-1):\n",
    "        x += 1./2 * v * stepsize\n",
    "        v += (2./3 * stepsize * (gradient_pot_energy(x)\n",
    "            + stepsize**2./24\n",
    "            * np.matmul(hessian_log_prog(x),gradient_pot_energy(x))))\n",
    "        x += 1./2 * v * stepsize\n",
    "        v += 1./3 * stepsize * gradient_pot_energy(x)\n",
    "\n",
    "    x += 1./2 * v * stepsize\n",
    "    v += (2./3 * stepsize * (gradient_pot_energy(x)\n",
    "        + stepsize**2./24\n",
    "        * np.matmul(hessian_log_prog(x),gradient_pot_energy(x))))\n",
    "    x += 1./2 * v * stepsize\n",
    "    v += 1./6 * stepsize * gradient_pot_energy(x)\n",
    "\n",
    "    return x, v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a652c078",
   "metadata": {},
   "source": [
    "Bare in mind, that higher accuracy, achieved with the U7, comes with a non negalectable additional computational cost. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-arena",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "broad-bloom",
   "metadata": {},
   "source": [
    "Imagine $(x^\\star,v^\\star)$ would be the exact solution after time $t$ and $(x_{stepsize},v_{stepsize})$ an approximation (keep in mind $t = stepsize * trajectory\\_length$), then we would say, that the methode (leapfrog or U7) is of n-th order and write $\\mathcal{O}(stepsize^n)$, if $||(x^\\star,v^\\star)-(x_{stepsize},v_{stepsize})||\\leq C * stepsize^n$ and $C$ is independend of the $stepsize.$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-standing",
   "metadata": {},
   "source": [
    "Note, the longer the stepsize, the larger the error and the less likely the acceptance of the proposed state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "negative-chicago",
   "metadata": {},
   "source": [
    "## Comparison \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2949af1f",
   "metadata": {},
   "source": [
    "As mentioned in the beginning, our goal was, to compare the leapfrog with the U7 to draw samples from a probability distribution. For this purpose we chose two different toy examples and two methods of convergence diagnostics to measure the fit of the drawn samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e704e9cb",
   "metadata": {},
   "source": [
    "The first example is a 100 dimensional standard normal distribution. We fixed the time to 10 and plotted the acceptance rate for different stepsizes against each other. The expected higher acceptance rate, due to a better approximation of the deterministic behaviour, can easily be observed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a6762a",
   "metadata": {},
   "source": [
    "![title](Figure_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06a2861",
   "metadata": {},
   "source": [
    "Unexpected peak at stepsize 1,5 might be explained by the hidden hamiltonian, but we could not figure out for sure, why this is the case. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc9f196",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e8c2c9c",
   "metadata": {},
   "source": [
    "summary: hessian sparse Zeitgewinn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
